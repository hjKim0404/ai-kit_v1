{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**_심층신경망(DNN) 기본기_**\n",
    "\n",
    "\n",
    "# 3. 퍼셉트론과 인공신경망의 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### _Objective_\n",
    "\n",
    "1. `인공신경망(Artificial Neural Network)`의 기본적인 개념과 구조, 표기법을 알아봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. `인공신경망(Artificial Neural Network)`의 가장 작은 단위인 `퍼셉트론(Perceptron)`의 기본 구조를 알아봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. 대표적인 `활성화함수(Activation Function)`와 함수 출력 결과의 형태에 대해 알아봅니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 필요한 패키지 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 시각화시 한글 폰트 설정을 위한 패키지\n",
    "# 시각화 부분에서 폰트 관련 문제 발생 시, 참고: https://jinyes-tistory.tistory.com/70\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "tag = HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    } else {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<a href=\"javascript:code_toggle()\">폰트설정 코드</a>''')\n",
    "display(tag)\n",
    "\n",
    "\n",
    "# plt 폰트 설정\n",
    "\n",
    "fm._rebuild() # 폰트 관련 캐시 삭제\n",
    "font_fname = '../../src/d2coding.ttf'\n",
    "font_family = fm.FontProperties(fname=font_fname).get_name()\n",
    "\n",
    "plt.rcParams[\"font.family\"] = font_family\n",
    "plt.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [1. 딥러닝과 신경망]\n",
    "\n",
    "---\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 동물 신경계의 신호처리 세포, `뉴런(Neuron; 신경세포)`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"./img/3_1.gif\" alt=\"2_30\" width=1200px>\n",
    "\n",
    "동물 신경망을 구성하는 뉴런은 아래와 같이 크게 세 부분으로 구성됨\n",
    "* 수상돌기(Dentrite) : 신호를 받아들이는 부분\n",
    "* 세포체(Nucueus) : 신호를 받아 다음 신호의 출력을 결정하는 부분\n",
    "* 축색돌기(Axon Terminal) : 출력 신호를 전달하는 부분\n",
    "\n",
    "인간의 두뇌에는 이렇게 생긴 뉴런 1000억 개가 네트워크로 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 뉴런에서 영감을 받은 `퍼셉트론`\n",
    "\n",
    "---\n",
    "\n",
    "`퍼셉트론 유닛(Perceptron Unit)`은\n",
    "1. 가장 간단한 형태의 선형분류기이다.\n",
    "2. 동물의 뉴런과 같이 입력신호를 받아 처리하여 결과를 낸다.\n",
    "3. 선형결합과 활성화함수의 연산을 처리한다.\n",
    "\n",
    "(이하`퍼셉트론`, `유닛(Unit)`,`인공뉴런`, `노드(Node)`, `뉴런(Neuron)` 모두 `퍼셉트론 유닛(Perceptron Unit)`을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 퍼셉트론 유닛(Perceptron Unit)\n",
    "<img src=\"./img/3_2.png\" alt=\"3_2\" width=2000px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z = w_1x_1 + w_2x_2 + b \\\\ y = h(z)$$\n",
    "\n",
    "※ 한 개의 유닛은 다수의 입력을 받아 한 개의 출력을 낸다. → 유닛의 수=출력의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### `입력신호(input; x)`\n",
    "\n",
    "$x_1,\\ x_2$: \n",
    "퍼셉트론의 외부로부터 전달받은 입력신호"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### `가중치(weight; w)`\n",
    "\n",
    "$w_1,\\ w_2$는 각 신호($x$)의 가중치를 나타내는 매개변수로, 각 신호의 영향력을 제어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### `편향 (bias; b)`\n",
    "\n",
    "$b$는 해당 퍼셉트론 유닛 내 선형결합 함수 $z(x)$의 위치를 조정 → 여러개의 퍼셉트론의 출력 수준에서 치우치지 않도록 제어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### `선형결합(linear Combination; z)`\n",
    "\n",
    "선형결합의 결과 `z` : 입력신호(Input signal) 과 가중치(Weight), 편향(Bias)에 의한 선형결합(Linear Combination)의 결과\n",
    "\n",
    "※ (일반적으로 선형결합의 결과 `z`는 분류문제에서 `로짓(logit)`으로 표기하므로 본 자료에서 편의상 모든 선형결합 `z`에 대해 `로짓(logit)`으로 표현)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### + 회귀모형과 퍼셉트론의 선형결합 비교\n",
    "\n",
    "`회귀모형(Regression Model)` : $\\hat{y} = \\beta_1x_1 + \\beta_2x_2 + \\varepsilon$\n",
    "\n",
    "`퍼셉트론(Perceptron)` : $z = w_1x_1 + w_2x_2 + b$\n",
    "\n",
    "회귀모형의 `종속변수`, `설명변수`, `회귀계수`, `오차항`이<br>\n",
    "퍼셉트론의 `로짓(logit; z)`, `입력신호(input signal)`, `가중치(weight)`, `편향(bias)`에 각각 대치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### `활성화 함수(Activation Function; h(z))`\n",
    "$h(z)$는 활성화 함수로, 입력 신호의 총합($z$)을 출력 신호($y$)로 변환하는 함수를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 활성화함수란\n",
    "\n",
    "`활성화함수(Activation Function)`은\n",
    "1. 선형결합의 결과값 `z`를 받아 출력신호(Output signal)로 변환하는 함수\n",
    "2. 비선형함수로 하여 인공신경망의 깊이(depth)에 효과를 부여\n",
    "3. 출력값의 범위와 의미를 부여하여 상황에 맞는 활성화함수를 사용\n",
    "\n",
    "\n",
    "$\n",
    "z = w_0 + w_1x_1 + w_2x_2 \\\\\n",
    "y = h(z) \\\\\n",
    "$\n",
    "\n",
    "$h(z)$가 활성화 함수를 의미<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `렐루(ReLu)` \n",
    "\n",
    "$\n",
    "RELU(x) = \\begin{cases}\n",
    "1 & \\mbox{if } x\\ge0 \\\\ \n",
    "0 & \\mbox{if } x\\lt0 \\\\ \n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "음의 결과를 0으로 출력하는 활성화함수, `렐루(ReLu)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "x = np.linspace(-20.0,20.0,101)\n",
    "y = relu(x)\n",
    "\n",
    "plt.title(\"relu Function\")\n",
    "plt.plot(x,y)\n",
    "plt.axhline(ls=\"--\",alpha=0.2)\n",
    "plt.axvline(ls=\"--\",alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ 딥러닝에서 렐루의 구현에서 $relu(z)  = max(z, 0)$이다.\n",
    "+ 출력값의 범위 : (0, $\\infty$)\n",
    "+ 기울기(Gradient) : 0 또는 1\n",
    "+ 은닉층(Hidden Layer)에서 주로 활용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "####  `시그모이드(Sigmoid)`\n",
    "\n",
    "${\\displaystyle S(x)={\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{e^{x}+1}}}$\n",
    "\n",
    "정규분포의 누적 분포 함수와 유사한 출력을 내는 활성화함수,`시그모이드(Sigmoid)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "x = np.linspace(-6.0,6.0,101)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.plot(x,y)\n",
    "plt.axhline(ls=\"--\",alpha=0.2)\n",
    "plt.axvline(ls=\"--\",alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ 딥러닝에서 시그모이드 활성화 함수를 $\\sigma(z)$로 표기한다.\n",
    "+ 출력값의 범위 : (0, 1)\n",
    "+ 기울기(Gradient)의 최대값 : 0.25\n",
    "+ Center : (0, 0.5)\n",
    "+ 정규분포의 누적 분포 함수와 유사한 함수이다.\n",
    "+ 특정 유닛의 결과값을 확률분포로 바꿀 필요가 있을때 활용한다.\n",
    "    + ex). 이진분류 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 케라스로 뉴런(Neuron) 구성하기\n",
    "\n",
    "---\n",
    "\n",
    "+ 데이터를 입력받는 `Input(Shape, name= )` 레이어\n",
    "+ 다수의 유닛으로 구성된 `Dense(units, activation=None, name= )` 레이어\n",
    "+ 정의된 입력과 출력으로 모델을 생성하는 `Model(inputs, output, name= )` 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3_3](./img/3_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "## 바로 위의 인공신경망을 구성하는 순서 셀은 녹화시 이번 h3에서만 설명 후, \n",
    "이후 예시를 만들 때에는 이미지가 없더라도 순서대로 진행해주세요. \n",
    "영상편집시 \"입력층의 형태를 결정하고\" 면 화면 우측 상단에 형태 결정 이 부분이 나오고,\n",
    "점점 코드를 칠때마다 그 다음 순서가 나오는 식으로 편집할 예정입니다.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "아직 \"층\"에 대한 개념이 없어서 \"다수의 뉴런으로 구성된 하나의 층\"으로 하려다가 바꿨습니다. \n",
    "name 인자를 name=(자동생성), name=(auto) 이헌식으로 표기하려다가, 잘 안되어서 비워두었습니다.\n",
    "입럭하지 않으면 자동생성됨을 말로 풀어서 알려주세요\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 문제 1. 한 개의 뉴런(Neuron) 구현하기\n",
    "\n",
    "<img src=\"./img/3_4.png\" alt=\"3_4\" width=\"1200\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# (1) 입력값의 형태 결정하기\n",
    "inputs = Input(1,name='x')\n",
    "\n",
    "# (2) 출력값의 형태 결정하기 \n",
    "dense_layer = Dense(1, name='output')\n",
    "\n",
    "# (3) 레이어를 연결하기\n",
    "output = dense_layer(inputs)\n",
    "\n",
    "# (4) 모델 구성하기\n",
    "model = Model(inputs, output, name='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "※ 이후, 본 자료에서 활성화함수를 특별히 명시하지 않을 경우 `유닛`은 `활성화함수가 RELU인 퍼셉트론 유닛`을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 문제 2.  두 개의 입력과 세 개의 뉴런(Neuron) 구현하기\n",
    "\n",
    "<img src=\"./img/3_5.png\" alt=\"3_5\" width=\"1200\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "처음 배울때 입력의 개수와 배치사이즈가 정말 헷갈립니다.\n",
    "여기서, 입력이 2개라는 것은 일반적인 태블러데이터에서 행의 수가 2개가 아니라 열의 수가 2개를 의미한다는 걸 말하고 싶었습니다.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 입력층 생성하기\n",
    "inputs = Input(shape=(2,))\n",
    "\n",
    "# 뉴런 생성하기\n",
    "neurons = Dense(3, activation='relu')\n",
    "\n",
    "# 입력층을 뉴런에 연결하기\n",
    "output = neurons(inputs)\n",
    "\n",
    "model = Model(inputs, output, name='neurons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "여기서 `입력이 2개`는 `두 개의 데이터`가 아니라 `하나의 데이터가 가진 두개의 피쳐(feature)`를 의미\n",
    "\n",
    "|예시)||피쳐1|피쳐2|\n",
    "|---|---|---|---|\n",
    "||**데이터1**|값|값|\n",
    "||**데이터2**|값|값|\n",
    "||...|...|...|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "※ `레이어(Layer)`: 특정 깊이에서 다수의 유닛으로 함께 연산하여 동시에 출력하는 하나의 층을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 인공신경망의 기본 구조\n",
    "\n",
    "---\n",
    "\n",
    "`인공신경망(Artificial Neural Network)`는 \n",
    "1. 다수의 퍼셉트론 유닛 결합으로 네트워크를 형성한 모델이다.\n",
    "2. 학습을 통해 퍼셉트론의 가중치를 변화시켜 문제 해결능력을 갖춘다.\n",
    "3. (주로) **여러개의 레이어간 입출력을 연결하여 구성한다.**\n",
    "4. 동물의 중추신경망을 모방한다.\n",
    "\n",
    "※ `레이어(Layer)`: 특정 깊이에서 다수의 유닛으로 함께 연산하여 동시에 출력하는 하나의 층을 의미\n",
    "\n",
    "( `인공신경망(Artificial Neural Network; ANN)`: /뉴럴넷/, `신경망`, `뉴럴네트워크`, `신경네트워크` 이하 동일) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**`인공신경망`의 레이어 구성**\n",
    "\n",
    "![3_6](./img/3_6.png)\n",
    "\n",
    "1. `입력층(Input Layer)` : 입력 신호를 받는 층(Layer)\n",
    "2. `은닉층(Hidden Layer)` : 입력층과 출력층 사이를 구성하는 층(Layer)\n",
    "3. `출력층(Output Layer)` : 출력 값을 반환하는 층(Layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 문제 3. 2층 신경망 구현하기\n",
    "\n",
    "![3_7](./img/3_7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 입력값의 형태 결정하기\n",
    "inputs = Input(shape=(2,),name='inputs')\n",
    "\n",
    "# 은닉층의 형태 결정하기\n",
    "hidden = Dense(3,activation='relu',name='hidden')(inputs)\n",
    "\n",
    "# 출력값의 형태 결정하기 \n",
    "outputs = Dense(1,activation='sigmoid',name='output')(hidden)\n",
    "\n",
    "# 모델 구성하기\n",
    "model = Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ 가중치(weight, $W^{[1]}, W^{[2]}$)를 가진 레이어는 은닉층에서 한 층, 출력층에서 한 층이므로 2층 신경망이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 문제 4. 5층 신경망 구현하기\n",
    "\n",
    "![3_8](./img/3_8.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 입력값의 형태 결정하기\n",
    "inputs = Input(shape=(2,),name='inputs')\n",
    "\n",
    "# 은닉층의 형태 결정하기\n",
    "hidden1 = Dense(3, activation='relu', name='hidden')(inputs)\n",
    "hidden2 = Dense(3, activation='relu', name='hidden1')(hidden1)\n",
    "hidden3 = Dense(3, activation='relu', name='hidden2')(hidden2)\n",
    "hidden4 = Dense(3, activation='relu', name='hidden3')(hidden3)\n",
    "\n",
    "# 출력값의 형태 결정하기 \n",
    "outputs = Dense(1, activation='sigmoid', name='output')(hidden4)\n",
    "\n",
    "# 모델 구성하기\n",
    "model = Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ `딥러닝(Deep Learning)`의 `딥(Deep)`은 이와같이 여러개의 층을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 문제 5. 다중 출력 신경망 구현하기\n",
    "\n",
    "![3_9](./img/3_9.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 입력값의 형태 결정하기\n",
    "inputs = Input(shape=(2,), name='inputs')\n",
    "\n",
    "# 은닉층의 형태 결정하기\n",
    "hidden1 = Dense(2, activation='relu', name='hidden1')(inputs)\n",
    "hidden2 = Dense(2, activation='relu', name='hidden2')(inputs)\n",
    "\n",
    "# 출력값의 형태 결정하기 \n",
    "output1 = Dense(2, activation='relu', name='output1')(hidden1)\n",
    "output2 = Dense(2, activation='relu', name='output2')(hidden2)\n",
    "\n",
    "# 모델 구성하기\n",
    "model = Model(inputs, [output1, output2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ 인공신경망은 이와같이 목적과 성능에 따라 \"층\"과 \"유닛\"으로 다양한 구성을 설계할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [2. 인공신경망의 유닛 수와 레이어  수의 의미]\n",
    "\n",
    "---\n",
    "\n",
    "padd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 인공신경망 `넓이`의 이점 : 유닛의 수\n",
    "\n",
    "---\n",
    "\n",
    "유닛의 수 변화에 따른 모델의 변화 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 예시 데이터\n",
    "\n",
    "일반적인 수학적 함수로는 근사하기 어려운 형태의 데이터 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4,4, 20001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = np.linspace(0,1,5000)**2 / 2 + 0.5\n",
    "y_2 = np.full(5000,-1)\n",
    "y_3 = np.full(5000, 1)\n",
    "y_4 = -np.linspace(-5,5,5001)**3 / 250 - 0.5\n",
    "y = np.concatenate([y_1,y_2,y_3,y_4]) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y,s=.05,c=\"C1\", label=\"data\")\n",
    "plt.legend(markerscale=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 복잡하게 구성된 데이터에 근사한 모델 만들기\n",
    "\n",
    "깊이가 2이고 은닉층의 활성화함수가 sigmoid인 네트워크에서 유닛의 수에 따른 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(621)\n",
    "\n",
    "# 유닛의 수에 따른 분류기 차이\n",
    "n_units = [1, 2, 10, 80]\n",
    "fig, axes = plt.subplots(2,2,figsize=(10,10))\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for n, ax in zip(n_units, axes):\n",
    "\n",
    "    # 깊이가2이며 유닛의 수가 n인 분류기\n",
    "    inputs = Input((1, ))\n",
    "    hidden = Dense(n, \"sigmoid\")(inputs)\n",
    "    outputs = Dense(1)(hidden)\n",
    "\n",
    "    # 구성된 네트워크로 모델 생성하기\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=SGD(0.1),\n",
    "                  loss=MeanSquaredError())\n",
    "\n",
    "    model.fit(x, y, epochs=20, verbose=0,batch_size=200)\n",
    "\n",
    "    ax.set_title(f\"유닛의 수가 {n}일때\")\n",
    "    ax.scatter(x,y,s=0.01,c=\"C1\", label=\"data\")\n",
    "    ax.plot(x,model.predict(x), label=\"model\")\n",
    "    ax.legend(loc=1,markerscale=40,fontsize='x-small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 보편 근사 정리 의미 (시벤코 정리; Cybenko's theorem)\n",
    "\n",
    "`시벤코 정리(Cybenko's theorem)`는\n",
    "\n",
    "1. \"유닛의 수가 충분하다면 은닉층 하나로 어떠한 함수도 원하는 정도로 근사할 수 있다\"\n",
    "2. 즉, 여러개의 유닛으로 구성된 인공신경망은 선형 회귀와 달리, **비선형 함수**도 근사할 수 있다\n",
    "3. 여러개의 유닛으로 구성된 인공신경망은 모형을 복잡하게 구성할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "![3_11](./img/3_11.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"./img/3_10.png\" alter=\"3_10\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**※ 단, ${\\mathbf  {w}}_{1},{\\mathbf  {w}}_{2},\\dots ,{\\mathbf  {w}}_{N},{\\mathbf  {\\alpha }}$와 ${\\mathbf  {\\theta }}$를 잘못 선택하거나 은닉층의 뉴런 수가 부족할 경우 충분한 정확도로 근사하는데 실패할 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "근사시키기 위해서 레이어의 수를 높이면서\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### 인공신경망 `깊이`의 이점  : 레이어의 수\n",
    "\n",
    "---\n",
    "\n",
    "그렇다면, 무조건 유닛의 수만 많으면 될까?<br>\n",
    "+ 유닛의 수만 많은 모델의 문제점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 문제점 1. XOR 문제\n",
    "\n",
    "아래의 네 점을 하나의 직선만으로 색깔을 구분할 수 있는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.array([0,0,1,1])\n",
    "x_2 = np.array([0,1,0,1])\n",
    "y = np.logical_xor(x_1,x_2)\n",
    "x = np.stack([x_1, x_2],axis=-1)\n",
    "\n",
    "print(np.logical_xor(x_1,x_2))\n",
    "\n",
    "plt.scatter(x_1,x_2,c=y,cmap=\"flag\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\"chalk board로 직선으로 분류하지 못하는 것을 보여주면 될 것 같아요.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 두개의 레이어로 구성할 경우\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "inputs = Input((2, ))\n",
    "hidden = Dense(20, \"relu\")(inputs)\n",
    "outputs = Dense(1,\"sigmoid\")(hidden)\n",
    "\n",
    "# 구성된 네트워크로 모델 생성하기\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=SGD(0.1),\n",
    "              loss=BinaryCrossentropy())\n",
    "model.fit(x, y, epochs=30, verbose=0)\n",
    "\n",
    "xx_1, xx_2  = np.indices((100,100)) / 100\n",
    "xx = np.stack([xx_1,xx_2],axis=-1).reshape(-1,2)\n",
    "p = model.predict(xx)\n",
    "p = p.reshape(xx_1.shape)\n",
    "p = np.where(p>0.5,1,0)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x_1,x_2,c=y,cmap=\"flag\")\n",
    "plt.imshow(p, alpha=0.2, cmap=\"jet\", extent=(-0.1,1.1,-0.1,1.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 문제점 2. 연산량\n",
    "\n",
    "파라미터($w$)의 수가 많으면 많을 수록 연산의 수는 급격하게 증가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "n_units 를 1부터 확인해 보면 될 것 같습니다.\n",
    "\n",
    "1일때는 레이어가 한 개일때, 직선 한개이므로 2개 영역으로 구분할 수 있다.\n",
    "2일때는 직선 두개 -> 4개\n",
    "3일때는 직선 세개 -> 7개\n",
    "\n",
    "레이어가 두 개일때,\n",
    "1일때 꺾은 선 한개 -> 2개\n",
    "2일때 꺾은 선 두개 -> 7개\n",
    "3일때 꺾은 선 세개 -> 17개\n",
    "\n",
    "레이어의 수 1개이고, 유닛의 수가 100개일때\n",
    ": 구분할 수 있는 영역 5051개, 파라미터의 수는 200개\n",
    "레이어의 수 2개이고, 유닛의 수가 11개일때\n",
    ": 구분할 수 있는 영역 5117개, 파라미터의 수는 154개뿐\n",
    "\n",
    "\n",
    "레이어의 수 1개이고, 유닛의 수가 200개일때\n",
    ": 구분할 수 있는 영역 20101개, 파라미터의 수는 400개\n",
    "레이어의 수 2개이고, 유닛의 수가 13개일때\n",
    ": 구분할 수 있는 영역 20477개, 파라미터의 수는 208개뿐\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 은닉층의 수가 1개일때, 은닉층의 유닛수에 따른 가능한 영역의 수\n",
    "\n",
    "n_units = 200\n",
    "\n",
    "## 수학적으로 계산된 방법이다. 여기서는 중요치 않음\n",
    "n_areas = 1 + np.sum(np.arange(n_units+1))\n",
    "\n",
    "inputs = Input((1, ))\n",
    "outputs = Dense(n_units)(inputs) # 한 개의 층을 가진 신경망\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "print(f\"# 레이어의 수가 한 개일때 :\")\n",
    "print(f\"구분가능한 영역의 최대 : {n_areas}\")\n",
    "print(f\"파라미터의 수 : {model.count_params()}\")\n",
    "print(f\"유닛의 수 : {n_units}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 은닉층 수가 2개일때, 은닉층의 같은 유닛수에 따른 가능한 영역의 수\n",
    "\n",
    "n_units = 13\n",
    "\n",
    "## 수학적으로 계산된 방법입니다.\n",
    "n_areas = 2 ** (n_units-1)*5-3\n",
    "\n",
    "inputs = Input((1, ))\n",
    "hidden = Dense(n_units)(inputs) # 두 개의 층을 가진 신경망\n",
    "outputs = Dense(n_units)(hidden) \n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "print(f\"# 레이어의 수가 두 개일때 :\")\n",
    "print(f\"구분가능한 영역의 최대 : {n_areas}\")\n",
    "print(f\"파라미터의 수 : {model.count_params()}\")\n",
    "print(f\"유닛의 수 : {n_units}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 구분 영역의 수에 따른 필요한 파라미터의 수 비교\n",
    "units_1 = np.arange(1,80)\n",
    "areas_1 = np.array([1 + np.sum(np.arange(n+1)) for n in units_1])\n",
    "params_1 = units_1 * 2\n",
    "\n",
    "units_2 = np.arange(1,11)\n",
    "areas_2 = 2**(units_2-1)*5-3\n",
    "params_2 = np.array([np.sum(np.arange(n)+2)*2 for n in units_2])\n",
    "\n",
    "plt.plot(areas_1, params_1, label=\"1 layer\")\n",
    "plt.plot(areas_2, params_2, label=\"2 layer\")\n",
    "plt.title(\"영역을 구분하기 위해 필요한 파라미터의 수\")\n",
    "plt.xlabel(\"구분가능한 영역의 수\")\n",
    "plt.ylabel(\"파라미터의 수\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "보시면, 구분해야하는 영역의 수가 1750여개 이하일경우에는\n",
    "단일 층으로 진행하는 것이 오히려 연산량이 더 작다\n",
    "하지만, 구분해야하는 영역의 수가 그 이상일 경우에는 두개의 층으로 구성하는 것이 낫다.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 문제점 3. 과적합문제\n",
    "\n",
    "모델이 지나치게 주어진 데이터에만 근사되는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "아웃라이어가 존재하는 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4,4, 20001)\n",
    "y = ((x-1)**2 -1)* 1/3 + 1\n",
    "y[2000:3000] =  y[2000:3000]-1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y,s=.05,c=\"C1\", label=\"data\")\n",
    "plt.ylim(0, 10)\n",
    "plt.legend(markerscale=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "\n",
    "유닛의 수가 2, 20, 80일때의 모델 변화\n",
    "\n",
    "사실, 우리가 원하는건 유닛의 수가 20으로 나왔을때 정도면 되는데,\n",
    "유닛의 수가 많아지면 많아질수록 점점 아웃라이어 데이터를 처리하려고 모델이 강제됨\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(333)\n",
    "\n",
    "n_units = [2, 20, 80]\n",
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "for n_unit,ax in zip(n_units, axes):\n",
    "    inputs = Input((1, ))\n",
    "    hidden = Dense(n_unit, \"sigmoid\")(inputs)\n",
    "    outputs = Dense(1)(hidden)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=SGD(0.1),\n",
    "                  loss=MeanSquaredError())\n",
    "    model.fit(x, y, epochs=10, verbose=0,batch_size=200)\n",
    "    \n",
    "    ax.set_title(f\"유닛의 수가 {n_unit}일때\")\n",
    "    ax.scatter(x,y,s=0.01,c=\"C1\", label=\"data\")\n",
    "    ax.plot(x,model.predict(x), label=\"model\")\n",
    "    ax.legend(markerscale=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 인공경망의 깊이의 의미 1\n",
    "\n",
    "`인공신경망의 구조에서 너비와 깊이`는\n",
    "1. `유닛의 수(너비;width)`를 증가시키는 것은 과적합을 유발할 수 있다.\n",
    "2. `층의 수(깊이; depth)`를 증가시키는 것은 일반화 효과를 가져올 수 있다.\n",
    "3. 깊이 없는 너비는 효과가 없다\n",
    "\n",
    "![3_12](./img/3_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 인공신경망의 깊이의 의미 2\n",
    "\n",
    "\n",
    "![3_13](./img/3_13.png)\n",
    "\n",
    "`적은 수의 유닛으로 더욱 다양한 영역구분을 수행할 수 있다.\n",
    "즉, 비슷한 퍼포먼스를 수행할 때 더욱 적은 연산량이 소모된다.`\n",
    "\n",
    "```python\n",
    "\"(⚠︎ 본 이미지는 이해를 돕기위한 예시이며, 실제 모델에서는 상기와 같은 꺾은선은 쉽게 나타나지 않는다.)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 인공신경망의 깊이에 따른 효과\n",
    "\n",
    "<img src=\"./img/2_4.png\"/>\n",
    "\n",
    "Reference : On the number of Linear Regions of deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "여기서, 너비가 점점 넓어질수록 어느 순간부터 정확도가 오히려 떨어지는데,\n",
    "이것이 아까 보았던 과적합 문제라고 얘기해주면 될 것 같습니다.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 활성화함수 `비선형성`의 이점 : 비선형성에 대하여\n",
    "\n",
    "시그모이드(sigmoid), 렐루(ReLu)모두 비선형 함수이다.\n",
    "\n",
    "선형함수 : 그래프를 그렸을 때 직선의 형태를 띄는 함수 → 항상 $f(x+y) = f(x) + f(y)$ 이며 $f(\\alpha x) = \\alpha(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 선형함수의 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(x):\n",
    "    y = 2 * x + 3\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-20.0,20.0,101)\n",
    "y = linear_function(x)\n",
    "plt.axes()\n",
    "plt.plot(x,y)\n",
    "\n",
    "plt.title(\"linear Function\")\n",
    "plt.axvline(ls=\"--\", alpha=0.5, lw=.7)\n",
    "plt.axhline(ls=\"--\", alpha=0.5, lw=.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 비선형함수인 활성화함수 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_func(x):\n",
    "    return np.maximum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-20.0,20.0,101)\n",
    "acts = {\"relu\": relu_func, \"sigmoid\": sigmoid}\n",
    "\n",
    "fig, axes = plt.subplots( 1, 2, figsize=(12, 4))\n",
    "for ax, act in zip(axes, acts):\n",
    "    a = acts[act](x)\n",
    "    \n",
    "    ax.set_title(f\"{act} Function\")\n",
    "    ax.plot(x,a)\n",
    "    ax.axvline(ls=\"--\", alpha=0.5, lw=.7)\n",
    "    ax.axhline(ls=\"--\", alpha=0.5, lw=.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 활성화함수에 따른 모델의 결과 확인하기\n",
    "\n",
    "`activation=None` 일경우, 활성화함수 $h(x) = x$<br>\n",
    "`activation=\"sigmoid\"` 일경우, 활성화함수 $h(x) = \\sigma(x)$<br>\n",
    "`activation=\"relu\"` 일경우, 활성화함수 $h(x) = max(x,0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4,4, 20001)\n",
    "y_1 = np.linspace(0,1,5000)**2 / 2 + 0.5\n",
    "y_2 = np.full(5000,-1)\n",
    "y_3 = np.full(5000, 1)\n",
    "y_4 = -np.linspace(-5,5,5001)**3 / 250 - 0.5\n",
    "y = np.concatenate([y_1,y_2,y_3,y_4]) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 예시\n",
    "plt.scatter(x,y,s=.05,c=\"C1\", label=\"data\")\n",
    "plt.legend(markerscale=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "sigmoid에서 SGD 0.1\n",
    "None에서 SGD 0.1\n",
    "비교.\n",
    "\n",
    "위에서 배운 sigmoid에서 활성화함수만 선형함수로 바꾸어 보자 ! 해서\n",
    "None과 sigmoid만 비교해도 될 것 같긴 함\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(10)\n",
    "\n",
    "# 선형함수와 비선형함수일때 비교\n",
    "acts = {\"linear\": None, \"sigmoid\": \"sigmoid\"}\n",
    "\n",
    "# 동일한 모델에서 비교\n",
    "fig, axes = plt.subplots( 1, 2, figsize=(12, 4))\n",
    "for ax, act in zip(axes, acts):\n",
    "    inputs = Input((1, ))\n",
    "    hidden = Dense(80, acts[act])(inputs)\n",
    "    outputs = Dense(1)(hidden)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=SGD(0.1),\n",
    "                  loss=MeanSquaredError())\n",
    "    model.fit(x, y, epochs=20, verbose=0,batch_size=400)\n",
    "    \n",
    "    ax.scatter(x,y,s=0.01,c=\"C1\", label=\"data\")\n",
    "    ax.set_title(f\"같은 조건에서 {act} 일때\")\n",
    "    ax.plot(x,model.predict(x), label=\"model\")\n",
    "    ax.legend(markerscale=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 선형함수를 활성화함수로 하면 왜 깊이의 효과가 없을까?\n",
    "\n",
    "**선형함수 $f(x) = ax + b$의 경우**\n",
    "\n",
    "예시)<br>\n",
    "$h(x) = 2x + 3$ 일 경우, 두번째 층에서의 출력값 $g(x) = h(h(x))$ 라고 하면,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# x : (-10, 10)\n",
    "x = np.linspace(-10,10,21)\n",
    "\n",
    "# y_1 : h(x) = 2x+3\n",
    "y_1 = 2*x + 3\n",
    "\n",
    "# y_2 : g(x) = h(h(x))\n",
    "y_2 = 2*y_1 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y_1,label=\"h(x)\",c=\"C0\")\n",
    "plt.plot(x,y_2, label=\"h(h(x))\",c=\"C1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$\n",
    "g(x) = h(h(x)) \\\\\n",
    "\\ \\ \\ \\ = 2(2x+3)+3 \\\\\n",
    "\\ \\ \\ \\ = 4x + 9\n",
    "$\n",
    "\n",
    "그러므로, 아무리 레이어를 연결하여도 선형 함수이기 때문에 깊이의 효과를 얻지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### + 선형 함수로 모델을 만들었을 떄 깊이에 따른 결과 변화\n",
    "\n",
    "비선형 함수 $h(x) = ax + b$에서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def linear_function(x):\n",
    "    y = 2 * x + 3\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(4)\n",
    "np.random.seed(100)\n",
    "\n",
    "inputs = Input((1,))\n",
    "hidden = Dense(5, linear_function)(inputs)\n",
    "hidden = Dense(5, linear_function)(hidden)\n",
    "outputs = Dense(2, linear_function)(hidden)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# 편향 가정\n",
    "for layer in model.layers[1:]:\n",
    "    layer.bias = np.random.uniform(-2, 2, layer.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = np.linspace(-10,10,21).reshape(-1,1)\n",
    "x = model.layers[0](data)\n",
    "h_1 = model.layers[1].call(x)\n",
    "h_2 = model.layers[2].call(h_1)\n",
    "h_3 = model.layers[3].call(h_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 활성화함수가 선형함수일때 각 층에서의 결과값 시각화\n",
    "fig, (ax1, ax2,ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(data,h_1)\n",
    "ax1.set_title(\"first layer\")\n",
    "\n",
    "ax2.plot(data,h_2)\n",
    "ax2.set_title(\"second layer\")\n",
    "\n",
    "ax3.plot(data,h_3)\n",
    "ax3.set_title(\"third layer\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**※ 층을 세 개 만들어 쌓았지만, 하나의 층이 있는 것과 동일하게 직선의 형태를 결과로 반환하게 되는 것** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### + 비선형 함수로 모델을 만들었을 떄 깊이에 따른 결과 변화\n",
    "비선형함수 ReLu $f(x) = max(x,0)$에서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(4)\n",
    "np.random.seed(100)\n",
    "\n",
    "inputs = Input((1,))\n",
    "hidden = Dense(5, \"relu\")(inputs)\n",
    "hidden = Dense(5, \"relu\")(hidden)\n",
    "outputs = Dense(2)(hidden)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "for layer in model.layers[1:]:\n",
    "    layer.bias = np.random.uniform(-2, 2, layer.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.linspace(-10,10,21).reshape(-1,1)\n",
    "x = model.layers[0](data)\n",
    "h_1 = model.layers[1].call(x)\n",
    "h_2 = model.layers[2].call(h_1)\n",
    "h_3 = model.layers[3].call(h_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 활성화함수가 비선형함수일때 각 층에서의 결과값 시각화\n",
    "fig, (ax1, ax2,ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(x,h_1)\n",
    "ax1.set_title(\"first layer\")\n",
    "\n",
    "ax2.plot(x,h_2)\n",
    "ax2.set_title(\"second layer\")\n",
    "\n",
    "ax3.plot(x,h_3)\n",
    "ax3.set_title(\"third layer\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "반면, 비선형 함수는 층을 쌓을 수록 직선 외의 다양한 곡선의 형태를 만들어 낼 수 있다는 특징을 가진다.\n",
    "\n",
    "**⚠︎ 주의 :**\n",
    "+ 활성화함수의 비선형성이 인공신경망에 비선형성을 부여하는 것은 아니다.\n",
    "+ 인공신경망에 비선형성을 부여하는 것은 너비(width)에 해당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [3. 인공신경망의 세부요소]\n",
    "\n",
    "---\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 인공신경망(ANN)의 세부 구성요소\n",
    "\n",
    "---\n",
    "\n",
    "`입력(input, x)`<br>\n",
    "`레이어(Layer; l)`와 `유닛(Unit; a)`<br>\n",
    "`가중치(Weight;b)`, `편향(Bias; b)` 과 `로짓(Logit;z)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 예시 네트워크\n",
    "\n",
    "\n",
    "![3_14](./img/3_14.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(3,),name='inputs')\n",
    "hidden = Dense(4, activation='relu',name='hidden')(inputs)\n",
    "outputs = Dense(1,activation='sigmoid',name='output')(hidden)\n",
    "\n",
    "model = Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 신경망 모델의 구조 확인하기\n",
    "\n",
    "\n",
    "\n",
    "![3_14](./img/3_14.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 신경망의 레이어 가져오기\n",
    "\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 신경망 모델의 입력 가져오기\n",
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 신경망 모델의 출력 가져오기\n",
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 신경망의 구조를 시각적으로 확인하기\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model,\n",
    "           show_shapes=True,\n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 신경망 모델의 각 레이어 확인하기\n",
    "\n",
    "\n",
    "\n",
    "![3_14](./img/3_14.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# 신경망의 레이어 가져오기\n",
    "\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 레이어의 이름 확인하기\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 첫번째 은닉층의 입력 가져오기\n",
    "model.layers[1].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 첫번째 은닉층의 유닛(출력) 가져오기\n",
    "model.layers[1].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 첫번째 은닉층의 유닛 수 가져오기\n",
    "model.layers[1].units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 첫번째 은닉층의 가중치(w) 확인하기\n",
    "model.layers[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 첫번째 은닉층의 활성화 함수 확인하기\n",
    "model.layers[1].activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 신경망 모델에서 구성요소의 표기\n",
    "\n",
    "---\n",
    "\n",
    "`입력(input, x)`<br>\n",
    "`레이어(Layer; l)`와 `유닛(Unit; a)`<br>\n",
    "`가중치(Weight;b)`, `편향(Bias; b)` 과 `로짓(Logit;z)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 레이어(Layer)와 유닛(Unit) 순번을 통한 위치 표기\n",
    "\n",
    "`레이어(Layer; l)` `유닛(Unit; a)`\n",
    "\n",
    "\n",
    "|층(Layer)의 위치|||\n",
    "|:---|:---|---:|\n",
    "|l번째 층 : |$a^{[l]}$|윗 첨자로 표기|\n",
    "\n",
    "|유닛(Unit)의 위치|||\n",
    "|:---|---|---|\n",
    "|j번째 유닛 : |$a_{j}$|아래 첨자로 표기|\n",
    "\n",
    "\n",
    "![3_15](./img/3_15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 신경망 모델의 입력 표기\n",
    "\n",
    "> $z = w_1x_1 + w_2x_2 + b\\ \\  ,\\ \\  a = h(z)$\n",
    "\n",
    "|입력된 데이터의 $i$ 번째 피쳐 ||  `입력` 벡터 표기|\n",
    "|:---:|---|---|\n",
    "|**$x_{i}$**||$X = $$\\begin{bmatrix} x_1 &  x_2 &  x_3 \\end{bmatrix}$|\n",
    "\n",
    "\n",
    "![3_16](./img/3_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 각 레이어의 유닛 표기\n",
    "\n",
    "> $z = w_1x_1 + w_2x_2 + b\\ \\  ,\\ \\  a = h(z)$\n",
    "\n",
    "\n",
    "|$l$번째 층의 $i$번째 `유닛` || $1$번째 층에서의 `유닛` 벡터 표기|\n",
    "|:---:|---|---|\n",
    "|**$a_{i}^{[l]}$**||$a^{[1]} = $ $\\begin{bmatrix} a^{[1]}_1 &  a^{[1]}_2 &  a^{[1]}_3 & a^{[1]}_4 \\end{bmatrix}$|\n",
    "\n",
    "![3_17](./img/3_17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 각 레이어의 커널(kernel) 표기\n",
    "\n",
    "> $z = w_1x_1 + w_2x_2 + b\\ \\  ,\\ \\  a = h(z)$\n",
    "\n",
    "각 층 별 가중치는 연결되어 있는 `(이전 층의 유닛 수, 다음 층의 유닛 수)`모양으로 존재\n",
    "\n",
    "* $W^{[1]}$의 shape : (입력층의 unit 수,은닉층의 unit 수) = (3,4)\n",
    "* $W^{[2]}$의 shape : (은닉층의 unit 수,출력층의 unit 수) = (4,1)\n",
    "\n",
    "※ 편향(bias)를 제외한 가중치($w_1$, $w_2$,...)를 `커널(kernel)`이라고도 한다.\n",
    "\n",
    "\n",
    "|$a_i^{[l-1]}$의 유닛에서 $a_j^{[l]}$의 유닛으로의 `커널` ||  $l$번째 층에서의 `커널` 벡터 표기|\n",
    "|:---:|---|---|\n",
    "|**$w_{i, j}^{[l]}$**||$W^{[1]} = $ $\\begin{bmatrix} \n",
    "w^{[1]}_{1,1} &  w^{[1]}_{1,2} &  w^{[1]}_{1,3} &  w^{[1]}_{1,4} \\\\\n",
    "w^{[1]}_{2,1} &  w^{[1]}_{2,2} &  w^{[1]}_{2,3} &  w^{[1]}_{2,4} \\\\\n",
    "w^{[1]}_{3,1} &  w^{[1]}_{3,2} &  w^{[1]}_{3,3} &  w^{[1]}_{3,4} \\\\\\end{bmatrix}$|\n",
    "\n",
    "\n",
    "\n",
    "![3_18](./img/3_18.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 각 유닛의 편향(bias) 표기\n",
    "\n",
    "> $z = w_1x_1 + w_2x_2 + b\\ \\  ,\\ \\  a = h(z)$\n",
    "\n",
    "각 유닛 별 편향은 하나씩 존재하므로, **각 층의 편향은 층의 유닛수와 동일**\n",
    "\n",
    "* $b^{[1]}$의 shape : (은닉층의 unit 수) = (4,)\n",
    "* $b^{[2]}$의 shape : (출력층의 unit 수) = (1,)\n",
    "\n",
    "\n",
    "\n",
    "|$l$ 번째 층에서 $a_j$ 유닛의 편향(Bias)|| $1$번째 층에서의 `편향` 벡터 표기|\n",
    "|:---:|---|---|\n",
    "|**$b_{j}^{[l]}$**||$b^{[1]} = $ $\\begin{bmatrix} b^{[1]}_1 &  b^{[1]}_2 &  b^{[1]}_3 & b^{[1]}_4 \\end{bmatrix}$|\n",
    "\n",
    "\n",
    "\n",
    "![3_19](./img/3_19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 각 유닛의 로짓(z) 표기\n",
    "\n",
    "> $z = w_1x_1 + w_2x_2 + b\\ \\  ,\\ \\  a = h(z)$<br>\n",
    "> 선형결합의 결과값(z)를 편의상 로짓(Logit)이라고 명명\n",
    "\n",
    "|$l$ 번째 층에서 $a_j$ 유닛의 로짓(Logit): |  \n",
    "|:---:|\n",
    "|**$z_{j}^{[l]}$**|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|$l$ 번째 층에서 $a_j$ 유닛의 로짓(Logit)|| $1$번째 층에서의 `로짓` 벡터 표기|\n",
    "|:---:|---|---|\n",
    "|**$z_{j}^{[l]}$**||$Z^{[1]} = $ $\\begin{bmatrix} z^{[1]}_1 &  z^{[1]}_2 &  z^{[1]}_3 & z^{[1]}_4 \\end{bmatrix}$|\n",
    "\n",
    "\n",
    "\n",
    "![3_20](./img/3_20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 각 구성요소들의 표기 정리\n",
    "\n",
    "> $z = w_1x_1 + w_2x_2 + b\\ \\  ,\\ \\  a = h(z)$<br>\n",
    "> `입력(input, x)`, `유닛(Unit; a)`, `커널(Kernel;w)`, `편향(Bias; b)`, `로짓(Logit;z)`\n",
    "\n",
    "![3_21](./img/3_21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `3. 퍼셉트론과 신경망의 구성` 마무리\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../../src/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "423.688px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
