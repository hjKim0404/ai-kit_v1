{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**_심층신경망(DNN) 기본기_**\n",
    "\n",
    "\n",
    "# 6. 인공신경망의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### _Objective_\n",
    "\n",
    "1. 각 가중치의 그래디언트 계산을 위한 순전파, 역전파의 알고리즘을 전개해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. 텐서플로우로 역전파 알고리즘를 직접 구현하여 인공신경망의 학습 과정을 이해합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. 케라스로 구현한 모델의 역전파를 진행하여 그 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 필요한 패키지 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [1. 역전파 알고리즘 전개하기 ]\n",
    "\n",
    "---\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 역전파(Backpropagation)\n",
    "\n",
    "---\n",
    "\n",
    "`역전파(Backpropagation)`는 \n",
    "1. 인공신경망의 학습을 위해 각 층의 가중치들의 그래디언트를 구하는 방법이다.\n",
    "2. 출력층에서 입력층으로 신호(Error Signal)을 전달하며 진행된다.\n",
    "3. 순전파에서 미리 계산해 둔 각 연산의 기울기를 이용한다.\n",
    "\n",
    "![](./img/neuron_imgs_4.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 순전파와 역전파 진행 순서\n",
    "\n",
    "![6_2](./img/6_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 순전파를 통한 각 층의 기울기 계산\n",
    "\n",
    "---\n",
    "\n",
    "각 측에서의  $\\frac{\\partial Z}{\\partial W}$, $\\frac{\\partial Z}{\\partial b}$, $\\frac{\\partial Z}{\\partial X}$<br>\n",
    "각 층에서의  $\\frac{\\partial a}{\\partial Z}$<br>\n",
    "손실에 대한  $\\frac{\\partial L}{\\partial Y_{pred}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide](./img/6_slide/6_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 학습에 필요한 그래디언트(Gradient)\n",
    "\n",
    "---\n",
    "\n",
    "각 층에서의 $\\frac{\\partial Loss}{\\partial W}$, $\\frac{\\partial Loss}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 역전파를 통한 그래디언트 계산\n",
    "\n",
    "---\n",
    "\n",
    "+ 마지막 층에서부터 계산된 손실에 대한  $\\frac{\\partial L}{\\partial \\hat{Y}}$ 부터 시작\n",
    "+ 한 층씩 전방으로 전파하여 미리 계산된 값과 함께 우리가 찾고자 하는 값을 계산<br>\n",
    "\"각 층에서의 $\\frac{\\partial Loss}{\\partial W}$, $\\frac{\\partial Loss}{\\partial b}$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 역전파 알고리즘 개괄\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 역전파를 통해 알아야 하는 점 (1) 학습 순서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_38.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 역전파를 통해 알아야 하는 점 (2) 메모리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![6_slide2](./img/6_slide/6_43.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [2. 텐서플로우를 활용한 역전파 구현하기]\n",
    "\n",
    "---\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  인공신경망 모델 구축하기\n",
    "![6_1](./img/6_1.png)\n",
    "\n",
    "+ 입력값 피쳐 수 2개 $x_1, x_2$\n",
    "+ 출력값 라벨 수 1개 $Y_{pred}$\n",
    "+ 은닉층 2개 $a^{[1]}, a^{[2]}$\n",
    "+ 은닉층의 유닛 수 3개\n",
    "+ 은닉층의 활성화 함수 : relu\n",
    "+ 출력층은 활성화 함수를 적용하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 샘플 데이터 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X = tf.constant([[1.,2.]])\n",
    "Y_true = tf.constant([[0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 임의의 가중치 세팅하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 첫번째 은닉층 : 세개의 유닛\n",
    "W1 = tf.Variable([\n",
    "    [0.5, 0.3, -0.4],\n",
    "    [0.3, -0.5, 0.3]]) ## (2, 3) 모양\n",
    "b1 = tf.Variable(\n",
    "    [0.3, -0.3, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 두번째 은닉층 : 세 개의 유닛\n",
    "W2 = tf.Variable([\n",
    "    [0.1, -0.2, 0.5],\n",
    "    [-0.2, 0.4, -0.3],\n",
    "    [0.5, 0.3, 0.4]]) ## (3, 3) 모양\n",
    "b2 = tf.Variable(\n",
    "    [0.1, -0.6, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 출력층 : 한 개의 유닛\n",
    "W3 = tf.Variable([\n",
    "    [0.1], [0.5], [-0.3]]) ## (3, 1) 모양\n",
    "b3 = tf.Variable([0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 첫번째 층에서의 순전파 진행\n",
    "\n",
    "---\n",
    "\n",
    "순전파 진행과 역전파를 위한 미분값 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 첫번째 층의 순전파\n",
    "\n",
    "$Z^{[1]} = X*W^{[1]} + b^{[1]}$, $a^{[1]} = \\max{(Z^{[1]},0)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(X)\n",
    "    \n",
    "    Z1 = X @ W1 + b1\n",
    "    a1 = tf.maximum(Z1, 0.)\n",
    "\n",
    "print(\"Z1 :\\n {}\".format(Z1))\n",
    "print(\"a1 :\\n {}\".format(a1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 첫번째 층에서의 미분값 계산\n",
    "$\\frac{\\partial Z^{[1]}}{\\partial W^{[1]}}$, $\\frac{\\partial Z^{[1]}}{\\partial b^{[1]}}$, $\\frac{\\partial Z^{[1]}}{\\partial X}$와\n",
    "$\\frac{\\partial a^{[1]}}{\\partial Z^{[1]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Z1_W1 = tape.gradient(Z1,W1)\n",
    "\n",
    "print(f\"dZ1/dW1 : \\n{grad_Z1_W1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Z1_b1 = tape.gradient(Z1,b1)\n",
    "\n",
    "print(f\"dZ1/db1 : {grad_Z1_b1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Z1_X = tape.gradient(Z1,X)\n",
    "\n",
    "print(f\"dZ1/dX : {grad_Z1_X}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_a1_Z1 = tape.gradient(a1, Z1)\n",
    "\n",
    "print(f\"da1/dZ1 : {grad_a1_Z1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 두번째 층에서의 순전파 진행\n",
    "\n",
    "---\n",
    "\n",
    "순전파 진행과 역전파를 위한 미분값 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 두번째 층의 순전파\n",
    "\n",
    "$Z^{[2]} = a^{[1]}*W^{[2]} + b^{[2]}$, $a^{[2]} = \\max{(Z^{[2]},0)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(a1)\n",
    "    \n",
    "    Z2 = a1 @ W2 + b2\n",
    "    a2 = tf.maximum(Z2, 0.)\n",
    "    \n",
    "print(\"Z2 :\\n {}\".format(Z2))\n",
    "print(\"a2 :\\n {}\".format(a2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 두번째 층에서의 미분값 계산\n",
    "$\\frac{\\partial Z^{[2]}}{\\partial W^{[2]}}$, $\\frac{\\partial Z^{[2]}}{\\partial b^{[2]}}$, $\\frac{\\partial Z^{[2]}}{\\partial a^{[1]}}$와\n",
    "$\\frac{\\partial a^{[2]}}{\\partial Z^{[2]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Z2_W2 = tape.gradient(Z2, W2)\n",
    "\n",
    "print(\"dZ2/dW2 : {}\".format(grad_Z2_W2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Z2_b2 = tape.gradient(Z2, b2)\n",
    "\n",
    "print(\"dZ2/db2 : {}\".format(grad_Z2_b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Z2_a1 = tape.gradient(Z2,a1)\n",
    "\n",
    "print(\"dZ2/da1 : {}\".format(grad_Z2_a1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_a2_Z2 = tape.gradient(a2, Z2)\n",
    "\n",
    "print(\"da2/dZ2 : {}\".format(grad_a2_Z2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 출력층에서의 순전파 진행\n",
    "\n",
    "---\n",
    "\n",
    "순전파 진행과 역전파를 위한 미분값 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 출력층의 순전파\n",
    "\n",
    "$Z^{[3]} = a^{[2]}*W^{[3]} + b^{[3]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(a2)\n",
    "    \n",
    "    Y_pred = a2 @ W3 + b3    \n",
    "    \n",
    "print(\"Y_pred :\\n {}\".format(Y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 출력층에서의 미분값 계산\n",
    "$\\frac{\\partial y_{pred}}{\\partial W^{[3]}}$, $\\frac{\\partial y_{pred}}{\\partial b^{[3]}}$, $\\frac{\\partial y_{pred}}{\\partial a^{[2]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Ypred_W3 = tape.gradient(Y_pred, W3)\n",
    "\n",
    "print(\"dY_pred/dW3 : {}\".format(grad_Ypred_W3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Ypred_b3 = tape.gradient(Y_pred, b3)\n",
    "\n",
    "print(\"dY_pred/db3 : {}\".format(grad_Ypred_b3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Ypred_a2 = tape.gradient(Y_pred, a2)\n",
    "\n",
    "print(\"dY_pred/da2 : {}\".format(grad_Ypred_a2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 손실함수 계산\n",
    "\n",
    "---\n",
    "\n",
    "손실함수 계산과 미분값 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 손실함수 계산 과정\n",
    "\n",
    "$Loss = \\frac{1}{n}\\sum_{i=1}^{n}( y_{pred} - y_{true})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(Y_pred)\n",
    "    Loss = tf.math.reduce_mean((Y_pred - Y_true)**2)\n",
    "    \n",
    "print(\"Loss :\\n {}\".format(Loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 손실함수 미분값 계산\n",
    "\n",
    "$\\frac{\\partial {Loss}}{\\partial {y_{pred}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Loss_Ypred = tape.gradient(Loss, Y_pred)\n",
    "\n",
    "print(\"dLoss/dY_pred : {}\".format(grad_Loss_Ypred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 역전파 진행\n",
    "\n",
    "---\n",
    "\n",
    "역전파를 진행하기 위해 전체 순전파 과정을 `tf.GradientTape`에 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(X)\n",
    "    \n",
    "    Z1 = X @ W1 + b1\n",
    "    a1 = tf.maximum(Z1, 0.)\n",
    "\n",
    "    Z2 = a1 @ W2 + b2\n",
    "    a2 = tf.maximum(Z2, 0.)\n",
    "    \n",
    "    Y_pred = a2 @ W3 + b3        \n",
    "    Loss = (Y_pred - Y_true)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 출력층에서의 역전파\n",
    "\n",
    "---\n",
    "\n",
    "순전파 과정에서 저장된 출력층에서의 기울기 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 출력층에서의 그래디언트 계산\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial W^{[3]}}$, $\\frac{\\partial Loss}{\\partial b^{[3]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Loss_W3 = tape.gradient(Loss, W3)\n",
    "grad_Loss_W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_value = grad_Loss_Ypred * grad_Ypred_W3\n",
    "\n",
    "tf.debugging.assert_equal(grad_Loss_W3, grad_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\" tape.gradient에서는 실제로 위와 같이 합성함수 미분법칙에 따라 계산합니다. \"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Loss_b3 = tape.gradient(Loss, b3)\n",
    "grad_Loss_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Loss_a2 = tape.gradient(Loss, a2)\n",
    "grad_Loss_a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 출력층에서의 가중치 갱신\n",
    "\n",
    "$W^{[3]} := W^{[3]} - \\lambda \\frac{\\partial Loss}{\\partial W^{[3]}}$, \n",
    "$b^{[3]} := b^{[3]} - \\lambda \\frac{\\partial Loss}{\\partial b^{[3]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W3.assign_sub(lr * grad_Loss_W3)\n",
    "W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b3.assign_sub(lr * grad_Loss_b3)\n",
    "b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 두번째 층에서의 역전파\n",
    "\n",
    "---\n",
    "\n",
    "순전파 과정에서 저장된 두번째 층에서의 기울기 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 두번째 층에서의 그래디언트 계산\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial W^{[2]}}$, $\\frac{\\partial Loss}{\\partial b^{[2]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Loss_Z2 = tape.gradient(Loss, Z2)\n",
    "grad_Loss_Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_value = grad_Loss_a2 * grad_a2_Z2\n",
    "\n",
    "tf.debugging.assert_equal(grad_Loss_Z2, grad_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\" tape.gradient에서는 실제로 위와 같이 합성함수 미분법칙에 따라 계산합니다. \"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Loss_W2 = tape.gradient(Loss, W2)\n",
    "grad_Loss_b2 = tape.gradient(Loss, b2)\n",
    "grad_Loss_a1 = tape.gradient(Loss, a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 두번째 층에서의 가중치 갱신\n",
    "\n",
    "$W^{[2]} := W^{[2]} - \\lambda \\frac{\\partial Loss}{\\partial b^{[2]}}$, $b^{[2]} := b^{[2]} - \\lambda \\frac{\\partial Loss}{\\partial b^{[2]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W2.assign_sub(lr * grad_Loss_W2)\n",
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b2.assign_sub(lr * grad_Loss_b2)\n",
    "b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 첫번째 층에서의 역전파\n",
    "\n",
    "---\n",
    "\n",
    "순전파 과정에서 저장된 첫번째 층에서의 기울기 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 첫번째 층에서의 그래디언트 계산\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial W^{[1]}}$, $\\frac{\\partial Loss}{\\partial b^{[1]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Loss_Z1 = tape.gradient(Loss, Z1)\n",
    "grad_Loss_Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_value = grad_Loss_a1 * grad_a1_Z1\n",
    "\n",
    "tf.debugging.assert_equal(grad_Loss_Z1, grad_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```python\n",
    "\" tape.gradient에서는 실제로 위와 같이 합성함수 미분법칙에 따라 계산합니다. \"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_Loss_W1 = tape.gradient(Loss, W1)\n",
    "grad_Loss_b1 = tape.gradient(Loss, b1)\n",
    "grad_Loss_X = tape.gradient(Loss, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 첫번째 층에서의 가중치 갱신\n",
    "\n",
    "\n",
    "$W^{[1]} := W^{[1]} - \\lambda \\frac{\\partial Loss}{\\partial b^{[1]}}$, $b^{[1]} := b^{[1]} - \\lambda \\frac{\\partial Loss}{\\partial b^{[1]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W1.assign_sub(lr * grad_Loss_W1)\n",
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b1.assign_sub(lr * grad_Loss_b1)\n",
    "b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [3. 케라스를 활용한 역전파 진행하기]\n",
    "\n",
    "---\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  인공신경망 모델 구축하기\n",
    "![6_1](./img/6_1.png)\n",
    "\n",
    "+ 입력값 피쳐 수 2개 $x_1, x_2$\n",
    "+ 출력값 라벨 수 1개 $Y_{pred}$\n",
    "+ 은닉층 2개 $a^{[1]}, a^{[2]}$\n",
    "+ 은닉층의 유닛 수 3개\n",
    "+ 은닉층의 활성화 함수 : relu\n",
    "+ 출력층은 활성화 함수를 적용하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "inputs = Input(2)\n",
    "dense_1 = Dense(3,\"relu\")(inputs)\n",
    "dense_2 = Dense(3, \"relu\")(dense_1)\n",
    "outputs = Dense(1)(dense_2)\n",
    "\n",
    "model = Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 학습이 진행될 모델의 가중치\n",
    "![6_3](./img/6_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "W1 = np.array([[0.5, 0.3, -0.4], [0.3, -0.5, 0.3]])\n",
    "b1 = np.array([0.3, -0.3, 0.2])\n",
    "\n",
    "W2 = np.array([[0.1, -0.2, 0.5],\n",
    "               [-0.2, 0.4, -0.3],\n",
    "               [0.5, 0.3, 0.4]])\n",
    "b2 = np.array([0.1, -0.6, 0.5])\n",
    "\n",
    "W3 = np.array([[0.1], [0.5],[-0.3]])\n",
    "b3 = np.array([0.5])\n",
    "\n",
    "model.set_weights([W1, b1, W2, b2, W3, b3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 모델로 역전파 진행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 모델의 순전파와 손실값 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    Y_pred = model(X)\n",
    "    Loss = tf.reduce_mean((Y_pred - Y_true)**2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 각 층에서의 그래디언트 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gradients = tape.gradient(Loss, model.trainable_weights)\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 그래디언트에 의한 가중치 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for weights, grad in zip(model.trainable_weights, gradients):\n",
    "    weights.assign_sub(lr * grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `6. 인공신경망의 역전파(Backpropagation)` 마무리\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../../src/logo.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "448px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
